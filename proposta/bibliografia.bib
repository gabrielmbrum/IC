@article{wu2023ctranscnn,
  title={CTransCNN: Combining transformer and CNN in multilabel medical image classification},
  author={Wu, Xin and Feng, Yue and Xu, Hong and Lin, Zhuosheng and Chen, Tao and Li, Shengke and Qiu, Shihan and Liu, Qichao and Ma, Yuangang and Zhang, Shuangsheng},
  journal={Knowledge-Based Systems},
  volume={281},
  pages={111030},
  year={2023},
  publisher={Elsevier}
}

@article{ribas2024color,
  title={Color-texture classification based on spatio-spectral complex network representations},
  author={Ribas, Lucas C and Scabini, Leonardo FS and Condori, Rayner HM and Bruno, Odemir M},
  journal={Physica A: Statistical Mechanics and its Applications},
  volume={635},
  pages={129518},
  year={2024},
  publisher={Elsevier}
}

@article{ribas2020fusion,
  title={Fusion of complex networks and randomized neural networks for texture analysis},
  author={Ribas, Lucas C and Junior, Jarbas Joaci de Mesquita S{\'a} and Scabini, Leonardo FS and Bruno, Odemir M},
  journal={Pattern Recognition},
  volume={103},
  pages={107189},
  year={2020},
  publisher={Elsevier}
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={Springer}
}

@misc{peng2021conformerlocalfeaturescoupling,
      title={Conformer: Local Features Coupling Global Representations for Visual Recognition}, 
      author={Zhiliang Peng and Wei Huang and Shanzhi Gu and Lingxi Xie and Yaowei Wang and Jianbin Jiao and Qixiang Ye},
      year={2021},
      eprint={2105.03889},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2105.03889}, 
}

@article{nanni2021comparison,
  title={Comparison of Different Image Data Augmentation Approaches},
  author={Nanni, Loris and Paci, Michelangelo and Brahnam, Sheryl and Lumini, Alessandra},
  journal={Journal of Imaging},
  volume={7},
  number={12},
  pages={254},
  year={2021},
  publisher={MDPI},
  doi={10.3390/jimaging7120254}
}

@article{nanni2022feature,
  title={Feature transforms for image data augmentation},
  author={Nanni, Loris and Paci, Michelangelo and Brahnam, Sheryl and Lumini, Alessandra},
  journal={Neural Computing and Applications},
  volume={34},
  pages={22345--22356},
  year={2022},
  publisher={Springer},
  doi={10.1007/s00521-022-07645-z}
}

@article{FRIDADAR2018321,
title = {GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification},
journal = {Neurocomputing},
volume = {321},
pages = {321-331},
year = {2018},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218310749},
author = {Maayan Frid-Adar and Idit Diamant and Eyal Klang and Michal Amitai and Jacob Goldberger and Hayit Greenspan},
keywords = {Image synthesis, Data augmentation, Convolutional neural networks, Generative adversarial network, Deep learning, Liver lesions, Lesion classification},
abstract = {Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results increased to 85.7% sensitivity and 92.4% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologistsâ€™ efforts to improve diagnosis.}
}

@article{ruiz2024gan,
  title={Enhancing Histopathological Image Classification Performance through Synthetic Data Generation with Generative Adversarial Networks},
  author={Ruiz-Casado, Jose L. and Molina-Cabello, Miguel A. and Luque-Baena, Rafael M.},
  journal={Sensors},
  volume={24},
  number={12},
  pages={3777},
  year={2024},
  publisher={MDPI},
  doi={10.3390/s24123777},
  url={https://www.mdpi.com/1424-8220/24/12/3777}
}

@article{hussain2025effresnet,
  title={EFFResNet-ViT: A Fusion-Based Convolutional and Vision Transformer Model for Explainable Medical Image Classification},
  author={Hussain, Tahir and Shouno, Hayaru and Hussain, Abid and others},
  journal={IEEE Access},
  volume={13},
  pages={54041--54066},
  year={2025},
  doi={10.1109/ACCESS.2025.3554184}
}

@article{chen2023medvit,
  title={MedViT: Hybrid CNN-Transformer Architecture for Robust Medical Image Diagnosis},
  author={Chen, Xingyu and Li, Rui and Wang, Zhiqiang and Wang, Tianfu},
  journal={Medical Image Analysis},
  volume={87},
  pages={102829},
  year={2023},
  publisher={Elsevier}
}

@misc{heusel2018ganstrainedtimescaleupdate,
      title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}, 
      author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
      year={2018},
      eprint={1706.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.08500}, 
}